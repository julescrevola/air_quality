{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframes\n",
    "path = \"../data/train.csv\"\n",
    "df = pd.read_csv(path, parse_dates=[0])\n",
    "\n",
    "df_weather1 = pd.read_csv(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/HOR/H_75_previous-2020-2022.csv.gz\", sep=';')\n",
    "df_weather2 = pd.read_csv(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/HOR/H_75_latest-2023-2024.csv.gz\", sep=';')\n",
    "weather = pd.concat([df_weather1, df_weather2])\n",
    "\n",
    "holidays_path = \"../data/fr-en-calendrier-scolaire.csv\"\n",
    "holidays = pd.read_csv(holidays_path, sep=\";\")\n",
    "\n",
    "traffic_path = \"../data/traffic_data.csv\"\n",
    "traffic = pd.read_csv(traffic_path, sep=\";\")\n",
    "\n",
    "final_test = pd.read_csv(\"../data/test.csv\", parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv(\"../data/test.csv\", parse_dates=[0]).rename(columns={\"id\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eng:\n",
    "    def __init__(self, df, weather, traffic, holidays):\n",
    "        self.df = df\n",
    "        self.df = self.df.rename(columns={\"id\": \"date\"})\n",
    "        self.df = self.df.drop_duplicates()\n",
    "\n",
    "        self.external_database(weather, traffic)\n",
    "        self.additional_data(holidays)\n",
    "        self.date_features()\n",
    "        \n",
    "\n",
    "    def external_database(self, weather, traffic):\n",
    "        \n",
    "        ##### Weather #####\n",
    "        \n",
    "        self.weather = weather\n",
    "\n",
    "        # Dropping empty columns\n",
    "        self.weather = self.weather.drop(\n",
    "            columns=['FF2', 'QFF2', 'DD2', 'QDD2', 'FXI2', 'QFXI2', 'DXI2', 'QDXI2', 'HXI2', 'QHXI2', 'DXI3S',\n",
    "                                      'DHUMEC', 'QDHUMEC', 'GEOP', 'QGEOP', 'N', 'QN', 'NBAS', 'QNBAS', 'CL', 'QCL', 'CM', 'QCM',\n",
    "                                      'CH', 'QCH', 'N1', 'QN1', 'C1', 'QC1', 'B1', 'QB1', 'N2', 'QN2', 'C2', 'QC2', 'B2', 'QB2',\n",
    "                                      'N3', 'QN3', 'C3', 'QC3', 'B3', 'QB3', 'N4', 'QN4', 'C4', 'QC4', 'B4', 'QB4', 'W1', 'QW1',\n",
    "                                      'W2', 'QW2', 'SOL', 'QSOL', 'SOLNG', 'QSOLNG', 'TMER', 'QTMER', 'VVMER', 'QVVMER', 'ETATMER',\n",
    "                                      'QETATMER', 'DIRHOULE', 'QDIRHOULE', 'HVAGUE', 'QHVAGUE', 'PVAGUE', 'QPVAGUE', 'HNEIGEF', 'QHNEIGEF',\n",
    "                                      'TSNEIGE', 'QTSNEIGE', 'TUBENEIGE', 'QTUBENEIGE', 'HNEIGEFI3', 'QHNEIGEFI3', 'HNEIGEFI1', 'QHNEIGEFI1',\n",
    "                                      'ESNEIGE', 'QESNEIGE', 'CHARGENEIGE', 'QCHARGENEIGE', 'DIR', 'QDIR', 'DIR2', 'QDIR2', 'DIF', 'QDIF',\n",
    "                                      'DIF2', 'QDIF2', 'UV', 'QUV', 'UV2', 'QUV2', 'UV_INDICE', 'QUV_INDICE', 'INFRAR', 'QINFRAR', 'INFRAR2',\n",
    "                                      'QINFRAR2', 'TLAGON', 'QTLAGON', 'TVEGETAUX', 'QTVEGETAUX', 'ECOULEMENT', 'QECOULEMENT', 'NUM_POSTE',\n",
    "                                      'NOM_USUEL', 'LAT', 'LON', 'DRR1', 'FF', 'QFF', 'DD', 'QDD', 'FXY', 'QFXY', 'DXY', 'QDXY', 'QHXY', 'FXI',\n",
    "                                      'QFXI', 'DXI', 'QDXI', 'QHXI', 'FXI3S', 'QFXI3S', 'QDXI3S', 'QHFXI3S', 'TD', 'QTD', 'T10', 'QT10', 'T20',\n",
    "                                      'QT20', 'T50', 'QT50', 'T100', 'QT100', 'TNSOL', 'QTNSOL', 'TN50', 'QTN50', 'TCHAUSSEE', 'QTCHAUSSEE',\n",
    "                                      'U', 'QU', 'UN', 'QUN', 'QHUN', 'UX', 'QUX', 'QHUX', 'DHUMI40', 'QDHUMI40', 'DHUMI80', 'QDHUMI80', 'TSV',\n",
    "                                      'QTSV', 'PMER', 'QPMER', 'PSTAT', 'QPSTAT', 'PMERMIN', 'QPMERMIN', 'VV', 'QVV', 'DVV200', 'QDVV200', 'WW',\n",
    "                                      'QWW', 'NEIGETOT', 'QNEIGETOT', 'GLO', 'QGLO', 'GLO2', 'QGLO2', 'INS', 'QINS', 'INS2', 'QINS2', 'QDRR1']\n",
    "                                      )\n",
    "        \n",
    "        # Grouping by date to sum and average values across all meteo stations\n",
    "        self.weather[\"date\"] = pd.to_datetime(self.weather[\"AAAAMMJJHH\"], format=\"%Y%m%d%H\")\n",
    "        self.weather = self.weather.drop(columns=[\"AAAAMMJJHH\", \"HXI\", \"HXY\", \"HFXI3S\", \"HTN\", \"HTX\", \"HUN\", \"HUX\"])\n",
    "\n",
    "        self.avg = self.weather.drop(\n",
    "            columns=['RR1']\n",
    "            ).groupby(\"date\").mean().reset_index()\n",
    "\n",
    "        self.tot = self.weather.drop(\n",
    "            columns=['TX', 'T', 'QTX', 'DG', 'QT', 'QHTN', 'QHTX', 'TN', 'QDG', 'QTN']\n",
    "            ).groupby(\"date\").sum().reset_index()\n",
    "\n",
    "        self.weather = self.weather.drop_duplicates(subset=[\"date\"])\n",
    "\n",
    "        self.df = self.df.merge(self.weather, on=\"date\", how=\"left\")\n",
    "\n",
    "        # Filling missing values for weather data with their mean as there are very few\n",
    "        self.is_na = self.df.isna().sum()\n",
    "\n",
    "        for col in self.is_na.index:\n",
    "            if self.is_na[col] > 0:\n",
    "                self.df[col] = self.df[col].fillna(self.df[col].mean())\n",
    "        \n",
    "\n",
    "        ##### Traffic #####\n",
    "        \n",
    "        # traffic.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "        traffic[\"date\"] = pd.to_datetime(traffic[\"date\"])\n",
    "\n",
    "        traffic = traffic.drop_duplicates(subset=[\"date\"])\n",
    "        linear_int2 = traffic[[\"date\", 'flow', 'occupation_rate']]\n",
    "        linear_int2 = linear_int2.set_index(\"date\")\n",
    "        new_features2 = linear_int2.resample('1h').interpolate(\"linear\")\n",
    "\n",
    "        self.df = self.df.merge(new_features2, on=\"date\", how=\"left\")\n",
    "\n",
    "        self.df[\"flow\"] = self.df[\"flow\"].fillna(self.df[\"flow\"].mean())\n",
    "        self.df[\"occupation_rate\"] = self.df[\"occupation_rate\"].fillna(self.df[\"occupation_rate\"].mean())\n",
    "        \n",
    "\n",
    "    def date_features(self):\n",
    "        # Create new date features\n",
    "        self.df['day'] = self.df['date'].dt.day\n",
    "        self.df['month'] = self.df['date'].dt.month\n",
    "        self.df['year'] = self.df['date'].dt.year\n",
    "        self.df['hour'] = self.df['date'].dt.hour\n",
    "        self.df['weekday'] = self.df['date'].dt.weekday \n",
    "        \n",
    "        self.df[\"weekend\"] = self.df['weekday'].isin([5, 6])\n",
    "        \n",
    "\n",
    "    def additional_data(self, holidays):\n",
    "    \n",
    "        ###### Lockdown variable ######\n",
    "        \n",
    "        # Lockdown in Paris\n",
    "        start_date = pd.to_datetime('2020-10-31')\n",
    "        end_date = pd.to_datetime('2020-12-14')\n",
    "        start_date1 = pd.to_datetime('2021-04-04')\n",
    "        end_date1 = pd.to_datetime('2021-05-02')\n",
    "        \n",
    "        # Assign 1 if the date is within the specified range, otherwise 0\n",
    "        self.df['lockdown'] = self.df['date'].apply(lambda x: 1 if ((start_date <= x <= end_date) or (start_date1 <= x <= end_date1)) else 0)\n",
    "        \n",
    "\n",
    "        ###### Curfews variable ######\n",
    "        \n",
    "        # Curfew date range (+1 hour because the data is the cumulative of the previous)\n",
    "        curfew_periods = [\n",
    "            (pd.to_datetime('2020-10-17 22:00'), pd.to_datetime('2020-10-29 07:00')),\n",
    "            (pd.to_datetime('2020-12-16 21:00'), pd.to_datetime('2021-01-15 07:00')),\n",
    "            (pd.to_datetime('2021-01-16 19:00'), pd.to_datetime('2021-03-20 07:00')),\n",
    "            (pd.to_datetime('2021-03-21 20:00'), pd.to_datetime('2021-04-02 07:00')),\n",
    "            (pd.to_datetime('2021-05-19 22:00'), pd.to_datetime('2021-06-08 07:00')),\n",
    "            (pd.to_datetime('2021-06-09 23:00'), pd.to_datetime('2021-06-20 07:00'))\n",
    "        ]\n",
    "\n",
    "        # Create a new column 'Curfew' and assign 1 if the date is within the specified range, otherwise 0\n",
    "\n",
    "        self.df['curfew'] = 0  # Initialize the Curfew column\n",
    "\n",
    "        # Loop through curfew periods and set Curfew column accordingly\n",
    "        for start_time, end_time in curfew_periods:\n",
    "            mask = self.df[self.df['date'].between(start_time, end_time)]\n",
    "            mask = mask[(mask[\"date\"].dt.hour > start_time.hour) | (mask[\"date\"].dt.hour < end_time.hour)]\n",
    "            self.df.loc[mask.index, 'curfew'] = 1\n",
    "            \n",
    "        \n",
    "        ###### Holidays variable ######\n",
    "\n",
    "        # Consider only metropolitan areas\n",
    "        holidays = holidays[holidays[\"Zones\"].isin([\"Zone C\"])]\n",
    "\n",
    "        # Consider only relevant years\n",
    "        holidays = holidays[holidays[\"annee_scolaire\"].isin([\"2020-2021\", \"2021-2022\"])]\n",
    "\n",
    "        # Distinguish for holidays in Paris or not\n",
    "        holidays['Holidays in Paris'] = 1\n",
    "\n",
    "        holidays.drop([\"Académies\",\"Population\", \"Zones\"], axis=1, inplace = True)\n",
    "\n",
    "        # Convert to same date format\n",
    "        holidays['Date de début'] = holidays['Date de début'].apply(lambda x: x[0:10]+' '+x[11:19])\n",
    "        holidays['Date de fin'] = holidays['Date de fin'].apply(lambda x: x[0:10]+' '+x[11:19])\n",
    "\n",
    "        holidays[\"Date de début\"] = pd.to_datetime(holidays[\"Date de début\"], format='%Y-%m-%d %H:%M:%S')\n",
    "        holidays[\"Date de fin\"] = pd.to_datetime(holidays[\"Date de fin\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Remove holidays starting after final date of dataset\n",
    "        holidays = holidays[holidays[\"Date de début\"].dt.year != 2022]\n",
    "\n",
    "        # Remove summer holidays\n",
    "        holidays = holidays[holidays[\"Description\"] != \"Vacances d'Été\"]\n",
    "        holidays.drop([\"Description\",\"annee_scolaire\"], axis=1, inplace=True)\n",
    "\n",
    "        # Drop same holidays\n",
    "        holidays = holidays.drop_duplicates(subset=['Date de début', 'Date de fin'], keep='first')\n",
    "\n",
    "        # Create holidays date ranges \n",
    "        ranges = []\n",
    "        for x in holidays[[\"Date de début\",\"Date de fin\"]].values:\n",
    "            ranges.append(pd.date_range(x[0], x[1], freq=\"h\"))\n",
    "            \n",
    "            \n",
    "        def is_date_within_ranges(date, ranges):\n",
    "            for date_range in ranges:\n",
    "                if date_range[0] <= date <= date_range[-1]:\n",
    "                    return 1\n",
    "            return 0\n",
    "\n",
    "        # Apply the function to create a new column indicating whether the date is within any holiday range\n",
    "        self.df['holiday'] = self.df['date'].apply(lambda x: is_date_within_ranges(x, ranges))\n",
    "        \n",
    "        \n",
    "        ###### Rush hour ########\n",
    "        def is_rush_hour(x):\n",
    "            if ((7 <= x.hour <= 9) or (16 <= x.hour <= 19)) and x.weekday != 5 and x.weekday != 6:\n",
    "                return 1\n",
    "            elif (x.weekday == 5 or x.weekday == 6) and (16 <= x.hour <= 19):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        self.df['rush_hour'] = self.df['date'].apply(lambda x: is_rush_hour(x))\n",
    "        \n",
    "\n",
    "    def output(self, test: bool=False):\n",
    "\n",
    "        if test == False:\n",
    "\n",
    "            # Using interpolation to fill in missing values before adding lag features\n",
    "            linear_int = self.df[[\"date\", \"valeur_NO2\", \"valeur_CO\", \"valeur_O3\", \"valeur_PM10\", \"valeur_PM25\"]]\n",
    "            linear_int = linear_int.set_index(\"date\")\n",
    "            new_features = linear_int.resample('1h').interpolate(\"linear\")\n",
    "\n",
    "            self.df = self.df.merge(new_features, on=\"date\", how=\"left\")\n",
    "            self.df = self.df.rename(columns={'valeur_NO2_y': 'valeur_NO2', 'valeur_CO_y': 'valeur_CO', 'valeur_O3_y': 'valeur_O3', 'valeur_PM10_y': 'valeur_PM10',\n",
    "                'valeur_PM25_y': 'valeur_PM25'})\n",
    "            self.df = self.df.drop(['valeur_NO2_x', 'valeur_CO_x', 'valeur_O3_x', 'valeur_PM10_x',\n",
    "                'valeur_PM25_x'], axis=1)\n",
    "            \n",
    "            # Generating lag columns\n",
    "            self.df[\"valeur_NO2_lag1\"] = self.df['valeur_NO2'].shift(1)\n",
    "            self.df[\"valeur_CO_lag1\"] = self.df['valeur_CO'].shift(1)\n",
    "            self.df[\"valeur_O3_lag1\"] = self.df['valeur_O3'].shift(1)\n",
    "            self.df[\"valeur_PM10_lag1\"] = self.df['valeur_PM10'].shift(1)\n",
    "            self.df[\"valeur_PM25_lag1\"] = self.df['valeur_PM25'].shift(1)\n",
    "\n",
    "            self.df[\"valeur_NO2_lag12\"] = self.df['valeur_NO2'].shift(12)\n",
    "            self.df[\"valeur_CO_lag12\"] = self.df['valeur_CO'].shift(12)\n",
    "            self.df[\"valeur_O3_lag12\"] = self.df['valeur_O3'].shift(12)\n",
    "            self.df[\"valeur_PM10_lag12\"] = self.df['valeur_PM10'].shift(12)\n",
    "            self.df[\"valeur_PM25_lag12\"] = self.df['valeur_PM25'].shift(12)\n",
    "\n",
    "            self.df[\"valeur_NO2_lag24\"] = self.df['valeur_NO2'].shift(24)\n",
    "            self.df[\"valeur_CO_lag24\"] = self.df['valeur_CO'].shift(24)\n",
    "            self.df[\"valeur_O3_lag24\"] = self.df['valeur_O3'].shift(24)\n",
    "            self.df[\"valeur_PM10_lag24\"] = self.df['valeur_PM10'].shift(24)\n",
    "            self.df[\"valeur_PM25_lag24\"] = self.df['valeur_PM25'].shift(24)\n",
    "\n",
    "            # Filling NaNs\n",
    "            self.df['valeur_NO2_lag1'] = self.df['valeur_NO2_lag1'].fillna(self.df['valeur_NO2'])\n",
    "            self.df['valeur_CO_lag1'] = self.df['valeur_CO_lag1'].fillna(self.df['valeur_CO'])\n",
    "            self.df['valeur_O3_lag1'] = self.df['valeur_O3_lag1'].fillna(self.df['valeur_O3'])\n",
    "            self.df['valeur_PM10_lag1'] = self.df['valeur_PM10_lag1'].fillna(self.df['valeur_PM10'])\n",
    "            self.df['valeur_PM25_lag1'] = self.df['valeur_PM25_lag1'].fillna(self.df['valeur_PM25'])\n",
    "\n",
    "            self.df['valeur_NO2_lag12'] = self.df['valeur_NO2_lag12'].fillna(self.df['valeur_NO2'])\n",
    "            self.df['valeur_CO_lag12'] = self.df['valeur_CO_lag12'].fillna(self.df['valeur_CO'])\n",
    "            self.df['valeur_O3_lag12'] = self.df['valeur_O3_lag12'].fillna(self.df['valeur_O3'])\n",
    "            self.df['valeur_PM10_lag12'] = self.df['valeur_PM10_lag12'].fillna(self.df['valeur_PM10'])\n",
    "            self.df['valeur_PM25_lag12'] = self.df['valeur_PM25_lag12'].fillna(self.df['valeur_PM25'])\n",
    "\n",
    "            self.df['valeur_NO2_lag24'] = self.df['valeur_NO2_lag24'].fillna(self.df['valeur_NO2'])\n",
    "            self.df['valeur_CO_lag24'] = self.df['valeur_CO_lag24'].fillna(self.df['valeur_CO'])\n",
    "            self.df['valeur_O3_lag24'] = self.df['valeur_O3_lag24'].fillna(self.df['valeur_O3'])\n",
    "            self.df['valeur_PM10_lag24'] = self.df['valeur_PM10_lag24'].fillna(self.df['valeur_PM10'])\n",
    "            self.df['valeur_PM25_lag24'] = self.df['valeur_PM25_lag24'].fillna(self.df['valeur_PM25'])\n",
    "\n",
    "            self.df.set_index(\"date\", inplace=True)\n",
    "        \n",
    "        else:\n",
    "            self.df.set_index(\"date\", inplace=True)\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, df, test: bool=False):\n",
    "        self.dfs = []\n",
    "        targets = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "        for col in targets:\n",
    "            if test == False:\n",
    "                self.df = df\n",
    "                self.X = self.df.drop(targets, axis=1) \n",
    "                for col2 in targets:\n",
    "                    if col2 == col:\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.X = self.X.drop([f'{col2}_lag1', f'{col2}_lag12', f'{col2}_lag24'], axis=1)\n",
    "                y = self.df[col]\n",
    "\n",
    "                self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, y, test_size=0.12, random_state=42, shuffle=False)\n",
    "\n",
    "                self.pipe(test=False, col=col)\n",
    "                self.dfs.append([self.data_train_prepared, self.data_test_prepared, self.y_train, self.y_test])\n",
    "        \n",
    "            else:\n",
    "                self.df = df\n",
    "                self.pipe(test=True, col=col)\n",
    "                self.dfs.append(self.data_pred)\n",
    "        \n",
    "        \n",
    "    def pipe(self, col: str, test: bool=False):\n",
    "        \n",
    "        categorical_features = ['year', 'holiday', 'rush_hour', 'lockdown', 'curfew']\n",
    "\n",
    "        numerical_features = ['weekend', f'{col}_lag1', f'{col}_lag12', f'{col}_lag24', 'flow',\n",
    "            'occupation_rate', 'ALTI', 'RR1', 'QRR1', 'T', 'QT', 'TN', 'QTN', 'QHTN', 'TX', 'QTX',\n",
    "            'QHTX', 'DG', 'QDG']\n",
    "\n",
    "        dates = [\"weekday\", \"month\", \"hour\", 'day']\n",
    "\n",
    "        cyclical = CyclicalFeatures(variables=None, drop_original=True)\n",
    "\n",
    "        full_pipeline = ColumnTransformer([\n",
    "                (\"cat\", OneHotEncoder(sparse_output=False), categorical_features),\n",
    "                (\"num\", MinMaxScaler(), numerical_features), \n",
    "                (\"dates\", cyclical, dates),\n",
    "            ], remainder='passthrough').set_output(transform='pandas')\n",
    "        \n",
    "        if test == False:\n",
    "            self.data_train_prepared = full_pipeline.fit_transform(self.X_train)\n",
    "            self.data_test_prepared = full_pipeline.transform(self.X_test)\n",
    "\n",
    "        else:\n",
    "            self.data_pred = full_pipeline.fit_transform(self.df)\n",
    "            self.data_pred['cat__curfew_1'] = 0\n",
    "            self.data_pred['cat__holiday_1'] = 0\n",
    "            self.data_pred['cat__lockdown_1'] = 0\n",
    "            self.data_pred['cat__year_2020'] = 0\n",
    "            self.data_pred['cat__year_2021'] = 0\n",
    "            self.data_pred['cat__year_2022'] = 0\n",
    "            self.data_pred['cat__year_2023'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = Eng(df, weather, traffic, holidays)\n",
    "df = eng.output(test=False)\n",
    "eng1 = Eng(final_test, weather, traffic, holidays)\n",
    "df_test = eng1.output(test=True)\n",
    "\n",
    "# Transforming values with log function to decrease influence of outliers\n",
    "for col in ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']:\n",
    "    df[col].apply(lambda x: np.log(x) if x > 0 else 0)\n",
    "\n",
    "# Shifting the lags by one position to initialize predictions\n",
    "lags = [1, 12, 24]\n",
    "for lag in lags:\n",
    "    df_test[f'valeur_NO2_lag{lag}'] = df[f'valeur_NO2_lag{lag}'].iloc[-1]\n",
    "    df_test[f'valeur_CO_lag{lag}'] = df[f'valeur_CO_lag{lag}'].iloc[-1]\n",
    "    df_test[f'valeur_O3_lag{lag}'] = df[f'valeur_O3_lag{lag}'].iloc[-1]\n",
    "    df_test[f'valeur_PM10_lag{lag}'] = df[f'valeur_PM10_lag{lag}'].iloc[-1]\n",
    "    df_test[f'valeur_PM25_lag{lag}'] = df[f'valeur_PM25_lag{lag}'].iloc[-1]\n",
    "\n",
    "data = Preprocess(df, test=False)\n",
    "data_test = Preprocess(df_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train = data.dfs\n",
    "dfs_test = data_test.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using recursive forecasting to update lags as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_forecast(df: pd.DataFrame, model, target_col: str, forecast_steps=504):\n",
    "\n",
    "    # Initialize predictions dictionary\n",
    "    predictions = []  # Store predictions\n",
    "\n",
    "    # Get a list of indices to ensure sequential row access by index\n",
    "    indices = df.index.tolist()\n",
    "    \n",
    "    for step in range(forecast_steps):\n",
    "        # Get the current row based on the index\n",
    "        current_index = indices[step]\n",
    "        latest_row = df.loc[current_index].copy()  # Copy to avoid modifying df directly\n",
    "        \n",
    "        # Prepare current input by selecting lagged features and other relevant columns\n",
    "        X_current = pd.DataFrame([latest_row])  # Single-row DataFrame with current features\n",
    "\n",
    "        if target_col == \"valeur_NO2\":\n",
    "            X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] = (X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] - 1.9)/ (131 - 1.9)\n",
    "        \n",
    "        elif target_col == \"valeur_CO\":\n",
    "            X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] =  (X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] - 0.037)/ (4.309 - 0.037)\n",
    "        \n",
    "        elif target_col == \"valeur_O3\":\n",
    "            X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] /= 193\n",
    "        \n",
    "        elif target_col == \"valeur_PM10\":\n",
    "            X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] = (X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] - 0.5)/ 128\n",
    "        \n",
    "        else:\n",
    "            X_current[[\n",
    "        f'num__{target_col}_lag1',\n",
    "        f'num__{target_col}_lag12', \n",
    "        f'num__{target_col}_lag24',\n",
    "    ]] /= 111.1\n",
    "    \n",
    "        # Predict all target variables at once\n",
    "        forecast_values = model.predict(X_current)[0] # Assuming model outputs a single array of predictions\n",
    "\n",
    "        # Store the predictions for this step\n",
    "        predictions.append(forecast_values)\n",
    "\n",
    "        # Update lagged values in the next row if it exists\n",
    "        if step < len(indices) - 1:  # Check if a next row exists\n",
    "            next_index = indices[step + 1]  # Find the next date index\n",
    "            index = indices[step]\n",
    "            # Shift lagged values backwards in the next row\n",
    "            \n",
    "            df.at[next_index, f'num__{target_col}_lag12'] = df.at[index, f'num__{target_col}_lag1']\n",
    "            df.at[next_index, f'num__{target_col}_lag24'] = df.at[index, f'num__{target_col}_lag12']\n",
    "            # Set the latest forecast as lag1 in the next row\n",
    "            df.at[next_index, f'num__{target_col}_lag1'] = forecast_values\n",
    "\n",
    "    return predictions, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'subsample': 1.0, 'min_child_samples': 10, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 500, 'depth': 8, 'colsample_bylevel': 0.8, 'border_count': 128, 'bagging_temperature': 1.0}\n",
      "Best Score:  -5.16041218629461\n",
      "Best Parameters:  {'subsample': 0.8, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 6, 'colsample_bylevel': 0.8, 'border_count': 128, 'bagging_temperature': 1.0}\n",
      "Best Score:  -0.04104316596408841\n",
      "Best Parameters:  {'subsample': 1.0, 'min_child_samples': 5, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 4, 'colsample_bylevel': 0.8, 'border_count': 64, 'bagging_temperature': 0.5}\n",
      "Best Score:  -6.382551693713932\n",
      "Best Parameters:  {'subsample': 0.8, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 4, 'colsample_bylevel': 1.0, 'border_count': 128, 'bagging_temperature': 1.5}\n",
      "Best Score:  -3.912205573175988\n",
      "Best Parameters:  {'subsample': 1.0, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 1250, 'depth': 6, 'colsample_bylevel': 1.0, 'border_count': 128, 'bagging_temperature': 1.5}\n",
      "Best Score:  -2.4745274576303067\n"
     ]
    }
   ],
   "source": [
    "############# NO NEED TO RUN AGAIN #############\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'depth': [4, 6, 8],\n",
    "    'iterations': [500, 1250, 2000],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0.5, 1.0, 1.5],\n",
    "    'border_count': [32, 64, 128],  # Number of splits for numerical features\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples used for training each tree\n",
    "    'colsample_bylevel': [0.8, 1.0],  # Fraction of features used for training each level\n",
    "    'min_child_samples': [1, 5, 10]  # Minimum number of samples in each leaf\n",
    "}\n",
    "\n",
    "best_params = []\n",
    "\n",
    "for i in dfs_train:\n",
    "    data_train_prepared  = i[0].drop(i[0].filter(regex='^remainder').columns, axis=1)\n",
    "    y_train = i[2]\n",
    "\n",
    "    catboost = CatBoostRegressor(verbose=False)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        catboost,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,  # Number of random combinations to try\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=TimeSeriesSplit(n_splits=6).split(data_train_prepared, y_train),  # Number of cross-validation folds\n",
    "    )\n",
    "\n",
    "    # Fit the RandomizedSearchCV object to the data\n",
    "    random_search.fit(data_train_prepared, y_train)  \n",
    "\n",
    "    # Print the best parameters and corresponding score\n",
    "    print(\"Best Parameters: \", random_search.best_params_)\n",
    "    print(\"Best Score: \", random_search.best_score_)\n",
    "\n",
    "    best_params.append(random_search.best_params_)\n",
    "\n",
    "############# NO NEED TO RUN AGAIN #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use {'subsample': 1.0, 'min_child_samples': 10, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 500, 'depth': 8, 'colsample_bylevel': 0.8, 'border_count': 128, 'bagging_temperature': 1.0} for valeur_NO2 \n",
      " Use {'subsample': 0.8, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 6, 'colsample_bylevel': 0.8, 'border_count': 128, 'bagging_temperature': 1.0} for valeur_CO \n",
      " Use {'subsample': 1.0, 'min_child_samples': 5, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 4, 'colsample_bylevel': 0.8, 'border_count': 64, 'bagging_temperature': 0.5} for valeur_O3 \n",
      " Use {'subsample': 0.8, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 500, 'depth': 4, 'colsample_bylevel': 1.0, 'border_count': 128, 'bagging_temperature': 1.5} for valeur_PM10 \n",
      " Use {'subsample': 1.0, 'min_child_samples': 1, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 1250, 'depth': 6, 'colsample_bylevel': 1.0, 'border_count': 128, 'bagging_temperature': 1.5} for valeur_PM25 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use these hyper-parameters for fitting the Catboost regressors to each target variable\n",
    "print(f'Use {best_params[0]} for valeur_NO2', \"\\n\",\n",
    "      f'Use {best_params[1]} for valeur_CO', \"\\n\",\n",
    "      f'Use {best_params[2]} for valeur_O3', \"\\n\",\n",
    "      f'Use {best_params[3]} for valeur_PM10', \"\\n\",\n",
    "      f'Use {best_params[4]} for valeur_PM25', \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jules\\AppData\\Local\\Temp\\ipykernel_13740\\3671204366.py:32: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  final_test = pd.concat([final_test, df_submit], axis=1)\n"
     ]
    }
   ],
   "source": [
    "final_test = pd.read_csv(\"../data/test.csv\", index_col=0, parse_dates=[0])\n",
    "targets = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']\n",
    "\n",
    "for i, j, k, l in zip(dfs_train, dfs_test, targets, best_params):\n",
    "    data_train_prepared  = i[0].drop(i[0].filter(regex='^remainder').columns, axis=1)\n",
    "    y_train = i[2]\n",
    "\n",
    "    data_test_prepared = i[1].drop(i[1].filter(regex='^remainder').columns, axis=1)\n",
    "    y_test = i[3]\n",
    "\n",
    "    data_pred = j.drop(j.filter(regex='^remainder').columns, axis=1)\n",
    "\n",
    "    # Define the specific index where the first row of `final_test_processed` is located\n",
    "    first_index = data_pred.index[0]  # Replace with the actual first index if known\n",
    "\n",
    "    # Extract the last three rows of `y_test`\n",
    "    last_three_rows = y_test[-3:]\n",
    "\n",
    "    # Assign lagged values to the specified row of `final_test_processed`\n",
    "    data_pred.at[first_index, f'num__{k}_lag24'] = last_three_rows.iloc[0]  # 3rd last row as lag3\n",
    "    data_pred.at[first_index, f'num__{k}_lag12'] = last_three_rows.iloc[1]  # 2nd last row as lag2\n",
    "    data_pred.at[first_index, f'num__{k}_lag1'] = last_three_rows.iloc[2]  # Last row as lag1\n",
    "        \n",
    "    forest = CatBoostRegressor(**l, verbose=False)\n",
    "    \n",
    "    miaou = forest.fit(data_train_prepared, y_train)\n",
    "\n",
    "    predictions, final_df = recursive_forecast(data_pred, miaou, target_col=k, forecast_steps=504)\n",
    "\n",
    "    df_submit = pd.DataFrame(predictions)\n",
    "\n",
    "    final_test = pd.concat([final_test, df_submit], axis=1)\n",
    "\n",
    "final_test = final_test[504:]\n",
    "final_test.columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test.csv\", index_col=0)\n",
    "final_test.index = test.index\n",
    "\n",
    "# Making sure no prediction is negative as it makes no sense\n",
    "for col in final_test.columns:\n",
    "    final_test[col].apply(lambda x: 0 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.to_csv(\"../output/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.concat([dfs_train[0][3], dfs_train[1][3], dfs_train[2][3], dfs_train[3][3], dfs_train[4][3]], axis=1)\n",
    "y_test.columns = ['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae for valeur_NO2 is 15.353618356083375\n",
      "mae for valeur_CO is 0.3345154653977376\n",
      "mae for valeur_O3 is 17.439246958140885\n",
      "mae for valeur_PM10 is 7.035681925923305\n",
      "mae for valeur_PM25 is 4.464356065545884\n",
      "Average MAE: 8.925483754218238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_mae(predictions, actuals, target_columns):\n",
    "    \"\"\"\n",
    "    Calculate the MAE for each pollutant and return the average MAE.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: dict of lists where keys are target columns and values are lists of predicted values\n",
    "    - actuals: DataFrame containing the actual values of the pollutants\n",
    "    - target_columns: List of target column names corresponding to pollutants\n",
    "    \n",
    "    Returns:\n",
    "    - average_mae: float, average MAE across the pollutants\n",
    "    \"\"\"\n",
    "    mae_values = []\n",
    "    \n",
    "    for target in target_columns:\n",
    "        # Calculate MAE for each pollutant\n",
    "        mae = mean_absolute_error(actuals[target], predictions[target])\n",
    "        mae_values.append(mae)\n",
    "        print(F\"mae for {target} is {mae}\")\n",
    "    \n",
    "    # Calculate the average MAE\n",
    "    average_mae = np.mean(mae_values)\n",
    "    return average_mae\n",
    "\n",
    "# Example usage\n",
    "target_columns = [\"valeur_NO2\", \"valeur_CO\", \"valeur_O3\", \"valeur_PM10\", \"valeur_PM25\"]\n",
    "average_mae = calculate_average_mae(final_test, y_test[len(y_test)-504:], target_columns)\n",
    "print(\"Average MAE:\", average_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
