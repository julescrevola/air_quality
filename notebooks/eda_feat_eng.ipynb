{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>valeur_NO2</th>\n",
       "      <th>valeur_CO</th>\n",
       "      <th>valeur_O3</th>\n",
       "      <th>valeur_PM10</th>\n",
       "      <th>valeur_PM25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.718</td>\n",
       "      <td>15.7</td>\n",
       "      <td>73.1</td>\n",
       "      <td>64.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.587</td>\n",
       "      <td>10.1</td>\n",
       "      <td>74.8</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>29.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.246</td>\n",
       "      <td>7.2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>25.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>29.3</td>\n",
       "      <td>0.204</td>\n",
       "      <td>8.3</td>\n",
       "      <td>15.3</td>\n",
       "      <td>13.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40986</th>\n",
       "      <td>2024-09-03 18:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222</td>\n",
       "      <td>55.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40987</th>\n",
       "      <td>2024-09-03 19:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.245</td>\n",
       "      <td>48.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40988</th>\n",
       "      <td>2024-09-03 20:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234</td>\n",
       "      <td>44.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40989</th>\n",
       "      <td>2024-09-03 21:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.225</td>\n",
       "      <td>25.9</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40990</th>\n",
       "      <td>2024-09-03 22:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.184</td>\n",
       "      <td>37.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40991 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  valeur_NO2  valeur_CO  valeur_O3  valeur_PM10  \\\n",
       "0     2020-01-01 00:00:00        42.9      0.718       15.7         73.1   \n",
       "1     2020-01-01 01:00:00        33.6      0.587       10.1         74.8   \n",
       "2     2020-01-01 02:00:00        29.3        NaN        5.1         51.0   \n",
       "3     2020-01-01 03:00:00        30.5      0.246        7.2         27.7   \n",
       "4     2020-01-01 04:00:00        29.3      0.204        8.3         15.3   \n",
       "...                   ...         ...        ...        ...          ...   \n",
       "40986 2024-09-03 18:00:00         NaN      0.222       55.1         12.0   \n",
       "40987 2024-09-03 19:00:00         NaN      0.245       48.2         13.4   \n",
       "40988 2024-09-03 20:00:00         NaN      0.234       44.5         12.4   \n",
       "40989 2024-09-03 21:00:00         NaN      0.225       25.9         10.6   \n",
       "40990 2024-09-03 22:00:00         NaN      0.184       37.7          8.5   \n",
       "\n",
       "       valeur_PM25  \n",
       "0             64.4  \n",
       "1             66.0  \n",
       "2             44.9  \n",
       "3             25.1  \n",
       "4             13.6  \n",
       "...            ...  \n",
       "40986          5.3  \n",
       "40987          7.0  \n",
       "40988          7.1  \n",
       "40989          5.4  \n",
       "40990          4.6  \n",
       "\n",
       "[40991 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/train.csv\"\n",
    "df = pd.read_csv(path, parse_dates=[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"id\": \"date\"})\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating year, month, date, hour, weekday number and weekend columns\n",
    "\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df[\"weekend\"] = df['weekday'].isin([5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date               0\n",
       "valeur_NO2      3297\n",
       "valeur_CO      12529\n",
       "valeur_O3        693\n",
       "valeur_PM10     7167\n",
       "valeur_PM25     1791\n",
       "day                0\n",
       "month              0\n",
       "year               0\n",
       "hour               0\n",
       "weekday            0\n",
       "weekend            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using interpolation to fill in missing values before adding lag features\n",
    "\n",
    "linear_int = df[[\"date\", \"valeur_NO2\", \"valeur_CO\", \"valeur_O3\", \"valeur_PM10\", \"valeur_PM25\"]]\n",
    "linear_int = linear_int.set_index(\"date\")\n",
    "new_features = linear_int.resample('1h').interpolate(\"linear\")\n",
    "\n",
    "df = df.merge(new_features, on=\"date\", how=\"left\")\n",
    "df = df.rename(columns={'valeur_NO2_y': 'valeur_NO2', 'valeur_CO_y': 'valeur_CO', 'valeur_O3_y': 'valeur_O3', 'valeur_PM10_y': 'valeur_PM10',\n",
    "       'valeur_PM25_y': 'valeur_PM25'})\n",
    "df = df.drop(['valeur_NO2_x', 'valeur_CO_x', 'valeur_O3_x', 'valeur_PM10_x',\n",
    "       'valeur_PM25_x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating lag columns\n",
    "\n",
    "df[\"valeur_NO2_lag1\"] = df['valeur_NO2'].shift(1)\n",
    "df[\"valeur_CO_lag1\"] = df['valeur_CO'].shift(1)\n",
    "df[\"valeur_O3_lag1\"] = df['valeur_O3'].shift(1)\n",
    "df[\"valeur_PM10_lag1\"] = df['valeur_PM10'].shift(1)\n",
    "df[\"valeur_PM25_lag1\"] = df['valeur_PM25'].shift(1)\n",
    "\n",
    "df[\"valeur_NO2_lag2\"] = df['valeur_NO2'].shift(12)\n",
    "df[\"valeur_CO_lag2\"] = df['valeur_CO'].shift(12)\n",
    "df[\"valeur_O3_lag2\"] = df['valeur_O3'].shift(12)\n",
    "df[\"valeur_PM10_lag2\"] = df['valeur_PM10'].shift(12)\n",
    "df[\"valeur_PM25_lag2\"] = df['valeur_PM25'].shift(12)\n",
    "\n",
    "df[\"valeur_NO2_lag3\"] = df['valeur_NO2'].shift(24)\n",
    "df[\"valeur_CO_lag3\"] = df['valeur_CO'].shift(24)\n",
    "df[\"valeur_O3_lag3\"] = df['valeur_O3'].shift(24)\n",
    "df[\"valeur_PM10_lag3\"] = df['valeur_PM10'].shift(24)\n",
    "df[\"valeur_PM25_lag3\"] = df['valeur_PM25'].shift(24)\n",
    "\n",
    "# Filling NaNs\n",
    "\n",
    "df['valeur_NO2_lag1'] = df['valeur_NO2_lag1'].fillna(df['valeur_NO2'])\n",
    "df['valeur_CO_lag1'] = df['valeur_CO_lag1'].fillna(df['valeur_CO'])\n",
    "df['valeur_O3_lag1'] = df['valeur_O3_lag1'].fillna(df['valeur_O3'])\n",
    "df['valeur_PM10_lag1'] = df['valeur_PM10_lag1'].fillna(df['valeur_PM10'])\n",
    "df['valeur_PM25_lag1'] = df['valeur_PM25_lag1'].fillna(df['valeur_PM25'])\n",
    "\n",
    "df['valeur_NO2_lag2'] = df['valeur_NO2_lag2'].fillna(df['valeur_NO2'])\n",
    "df['valeur_CO_lag2'] = df['valeur_CO_lag2'].fillna(df['valeur_CO'])\n",
    "df['valeur_O3_lag2'] = df['valeur_O3_lag2'].fillna(df['valeur_O3'])\n",
    "df['valeur_PM10_lag2'] = df['valeur_PM10_lag2'].fillna(df['valeur_PM10'])\n",
    "df['valeur_PM25_lag2'] = df['valeur_PM25_lag2'].fillna(df['valeur_PM25'])\n",
    "\n",
    "df['valeur_NO2_lag3'] = df['valeur_NO2_lag3'].fillna(df['valeur_NO2'])\n",
    "df['valeur_CO_lag3'] = df['valeur_CO_lag3'].fillna(df['valeur_CO'])\n",
    "df['valeur_O3_lag3'] = df['valeur_O3_lag3'].fillna(df['valeur_O3'])\n",
    "df['valeur_PM10_lag3'] = df['valeur_PM10_lag3'].fillna(df['valeur_PM10'])\n",
    "df['valeur_PM25_lag3'] = df['valeur_PM25_lag3'].fillna(df['valeur_PM25'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding holidays data\n",
    "\n",
    "holidays_path = \"../data/fr-en-calendrier-scolaire.csv\"\n",
    "holidays = pd.read_csv(holidays_path, sep=\";\")\n",
    "\n",
    "# Consider only metropolitan areas\n",
    "holidays = holidays[holidays[\"Zones\"].isin([\"Zone C\"])]\n",
    "\n",
    "# Consider only relevant years\n",
    "holidays = holidays[holidays[\"annee_scolaire\"].isin([\"2020-2021\", \"2021-2022\"])]\n",
    "\n",
    "# Distinguish for holidays in Paris or not\n",
    "holidays['Holidays in Paris'] = 1\n",
    "\n",
    "holidays.drop([\"Académies\",\"Population\", \"Zones\"], axis=1, inplace = True)\n",
    "\n",
    "# Convert to same date format\n",
    "holidays['Date de début'] = holidays['Date de début'].apply(lambda x: x[0:10]+' '+x[11:19])\n",
    "holidays['Date de fin'] = holidays['Date de fin'].apply(lambda x: x[0:10]+' '+x[11:19])\n",
    "\n",
    "holidays[\"Date de début\"] = pd.to_datetime(holidays[\"Date de début\"], format='%Y-%m-%d %H:%M:%S')\n",
    "holidays[\"Date de fin\"] = pd.to_datetime(holidays[\"Date de fin\"], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Remove holidays starting after final date of dataset\n",
    "holidays = holidays[holidays[\"Date de début\"].dt.year != 2022]\n",
    "\n",
    "# Remove summer holidays\n",
    "holidays = holidays[holidays[\"Description\"] != \"Vacances d'Été\"]\n",
    "holidays.drop([\"Description\",\"annee_scolaire\"], axis=1, inplace=True)\n",
    "\n",
    "# Drop same holidays\n",
    "holidays = holidays.drop_duplicates(subset=['Date de début', 'Date de fin'], keep='first')\n",
    "\n",
    "# Create holidays date ranges \n",
    "ranges = []\n",
    "for x in holidays[[\"Date de début\",\"Date de fin\"]].values:\n",
    "    ranges.append(pd.date_range(x[0], x[1], freq=\"h\"))\n",
    "    \n",
    "    \n",
    "def is_date_within_ranges(date, ranges):\n",
    "    for date_range in ranges:\n",
    "        if date_range[0] <= date <= date_range[-1]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Apply the function to create a new column indicating whether the date is within any holiday range\n",
    "df['holiday'] = df['date'].apply(lambda x: is_date_within_ranges(x, ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking into account rush hours for traffic\n",
    "\n",
    "def is_rush_hour(x):\n",
    "    if ((7 <= x.hour <= 9) or (16 <= x.hour <= 19)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['rush_hour'] = df['date'].apply(lambda x: is_rush_hour(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/traffic_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Using the traffic data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m traffic_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/traffic_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m traffic \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraffic_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m traffic\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m traffic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(traffic[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/traffic_data.csv'"
     ]
    }
   ],
   "source": [
    "# Using the traffic data\n",
    "\n",
    "traffic_path = \"../data/traffic_data.csv\"\n",
    "traffic = pd.read_csv(traffic_path, sep=\";\")\n",
    "\n",
    "traffic.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "traffic[\"date\"] = pd.to_datetime(traffic[\"date\"])\n",
    "\n",
    "traffic = traffic.drop_duplicates(subset=[\"date\"])\n",
    "linear_int2 = traffic[[\"date\", 'flow', 'occupation_rate']]\n",
    "linear_int2 = linear_int2.set_index(\"date\")\n",
    "new_features2 = linear_int2.resample('1h').interpolate(\"linear\")\n",
    "\n",
    "df = df.merge(new_features2, on=\"date\", how=\"left\")\n",
    "\n",
    "df[\"flow\"] = df[\"flow\"].fillna(df[\"flow\"].mean())\n",
    "df[\"occupation_rate\"] = df[\"occupation_rate\"].fillna(df[\"occupation_rate\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lockdown  and Curfew features\n",
    "We expect to have less air pollution during the lockdown Covid period and the curfews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lockdown date range\n",
    "start_date = pd.to_datetime('2020-10-31')\n",
    "end_date = pd.to_datetime('2020-12-14')\n",
    "start_date1 = pd.to_datetime('2021-04-04')\n",
    "end_date1 = pd.to_datetime('2021-05-02')\n",
    "\n",
    "# Create a new column and assign 1 if the date is within the specified range, otherwise 0\n",
    "df['lockdown'] = df['date'].apply(lambda x: 1 if ((start_date <= x <= end_date) or (start_date1 <= x <= end_date1)) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curfew date range \n",
    "curfew_periods = [\n",
    "    (pd.to_datetime('2020-10-17 22:00'), pd.to_datetime('2020-10-29 07:00')),\n",
    "    (pd.to_datetime('2020-12-16 21:00'), pd.to_datetime('2021-01-15 07:00')),\n",
    "    (pd.to_datetime('2021-01-16 19:00'), pd.to_datetime('2021-03-20 07:00')),\n",
    "    (pd.to_datetime('2021-03-21 20:00'), pd.to_datetime('2021-04-02 07:00')),\n",
    "    (pd.to_datetime('2021-05-19 22:00'), pd.to_datetime('2021-06-08 07:00')),\n",
    "    (pd.to_datetime('2021-06-09 23:00'), pd.to_datetime('2021-06-20 07:00'))\n",
    "]\n",
    "\n",
    "df['curfew'] = 0  \n",
    "\n",
    "# Loop through curfew periods and set Curfew column accordingly\n",
    "for start_time, end_time in curfew_periods:\n",
    "    mask = df[df['date'].between(start_time, end_time)]\n",
    "    mask = mask[(mask[\"date\"].dt.hour > start_time.hour) | (mask[\"date\"].dt.hour < end_time.hour)]\n",
    "    df.loc[mask.index, 'curfew'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather1 = pd.read_csv(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/HOR/H_75_previous-2020-2022.csv.gz\", sep=';')\n",
    "df_weather2 = pd.read_csv(\"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/HOR/H_75_latest-2023-2024.csv.gz\", sep=';')\n",
    "df_weather = pd.concat([df_weather1, df_weather2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column FF2 has 252815 missing values.\n",
      "Column QFF2 has 252815 missing values.\n",
      "Column DD2 has 252815 missing values.\n",
      "Column QDD2 has 252815 missing values.\n",
      "Column FXI2 has 252815 missing values.\n",
      "Column QFXI2 has 252815 missing values.\n",
      "Column DXI2 has 252815 missing values.\n",
      "Column QDXI2 has 252815 missing values.\n",
      "Column HXI2 has 252815 missing values.\n",
      "Column QHXI2 has 252815 missing values.\n",
      "Column DXI3S has 252815 missing values.\n",
      "Column DHUMEC has 252815 missing values.\n",
      "Column QDHUMEC has 252815 missing values.\n",
      "Column GEOP has 252815 missing values.\n",
      "Column QGEOP has 252815 missing values.\n",
      "Column N has 252815 missing values.\n",
      "Column QN has 252815 missing values.\n",
      "Column NBAS has 252815 missing values.\n",
      "Column QNBAS has 252815 missing values.\n",
      "Column CL has 252815 missing values.\n",
      "Column QCL has 252815 missing values.\n",
      "Column CM has 252815 missing values.\n",
      "Column QCM has 252815 missing values.\n",
      "Column CH has 252815 missing values.\n",
      "Column QCH has 252815 missing values.\n",
      "Column N1 has 252815 missing values.\n",
      "Column QN1 has 252815 missing values.\n",
      "Column C1 has 252815 missing values.\n",
      "Column QC1 has 252815 missing values.\n",
      "Column B1 has 252815 missing values.\n",
      "Column QB1 has 252815 missing values.\n",
      "Column N2 has 252815 missing values.\n",
      "Column QN2 has 252815 missing values.\n",
      "Column C2 has 252815 missing values.\n",
      "Column QC2 has 252815 missing values.\n",
      "Column B2 has 252815 missing values.\n",
      "Column QB2 has 252815 missing values.\n",
      "Column N3 has 252815 missing values.\n",
      "Column QN3 has 252815 missing values.\n",
      "Column C3 has 252815 missing values.\n",
      "Column QC3 has 252815 missing values.\n",
      "Column B3 has 252815 missing values.\n",
      "Column QB3 has 252815 missing values.\n",
      "Column N4 has 252815 missing values.\n",
      "Column QN4 has 252815 missing values.\n",
      "Column C4 has 252815 missing values.\n",
      "Column QC4 has 252815 missing values.\n",
      "Column B4 has 252815 missing values.\n",
      "Column QB4 has 252815 missing values.\n",
      "Column W1 has 252815 missing values.\n",
      "Column QW1 has 252815 missing values.\n",
      "Column W2 has 252815 missing values.\n",
      "Column QW2 has 252815 missing values.\n",
      "Column SOL has 252815 missing values.\n",
      "Column QSOL has 252815 missing values.\n",
      "Column SOLNG has 252815 missing values.\n",
      "Column QSOLNG has 252815 missing values.\n",
      "Column TMER has 252815 missing values.\n",
      "Column QTMER has 252815 missing values.\n",
      "Column VVMER has 252815 missing values.\n",
      "Column QVVMER has 252815 missing values.\n",
      "Column ETATMER has 252815 missing values.\n",
      "Column QETATMER has 252815 missing values.\n",
      "Column DIRHOULE has 252815 missing values.\n",
      "Column QDIRHOULE has 252815 missing values.\n",
      "Column HVAGUE has 252815 missing values.\n",
      "Column QHVAGUE has 252815 missing values.\n",
      "Column PVAGUE has 252815 missing values.\n",
      "Column QPVAGUE has 252815 missing values.\n",
      "Column HNEIGEF has 252815 missing values.\n",
      "Column QHNEIGEF has 252815 missing values.\n",
      "Column TSNEIGE has 252815 missing values.\n",
      "Column QTSNEIGE has 252815 missing values.\n",
      "Column TUBENEIGE has 252815 missing values.\n",
      "Column QTUBENEIGE has 252815 missing values.\n",
      "Column HNEIGEFI3 has 252815 missing values.\n",
      "Column QHNEIGEFI3 has 252815 missing values.\n",
      "Column HNEIGEFI1 has 252815 missing values.\n",
      "Column QHNEIGEFI1 has 252815 missing values.\n",
      "Column ESNEIGE has 252815 missing values.\n",
      "Column QESNEIGE has 252815 missing values.\n",
      "Column CHARGENEIGE has 252815 missing values.\n",
      "Column QCHARGENEIGE has 252815 missing values.\n",
      "Column DIR has 252815 missing values.\n",
      "Column QDIR has 252815 missing values.\n",
      "Column DIR2 has 252815 missing values.\n",
      "Column QDIR2 has 252815 missing values.\n",
      "Column DIF has 252815 missing values.\n",
      "Column QDIF has 252815 missing values.\n",
      "Column DIF2 has 252815 missing values.\n",
      "Column QDIF2 has 252815 missing values.\n",
      "Column UV has 252815 missing values.\n",
      "Column QUV has 252815 missing values.\n",
      "Column UV2 has 252815 missing values.\n",
      "Column QUV2 has 252815 missing values.\n",
      "Column UV_INDICE has 252815 missing values.\n",
      "Column QUV_INDICE has 252815 missing values.\n",
      "Column INFRAR has 252815 missing values.\n",
      "Column QINFRAR has 252815 missing values.\n",
      "Column INFRAR2 has 252815 missing values.\n",
      "Column QINFRAR2 has 252815 missing values.\n",
      "Column TLAGON has 252815 missing values.\n",
      "Column QTLAGON has 252815 missing values.\n",
      "Column TVEGETAUX has 252815 missing values.\n",
      "Column QTVEGETAUX has 252815 missing values.\n",
      "Column ECOULEMENT has 252815 missing values.\n",
      "Column QECOULEMENT has 252815 missing values.\n",
      "['FF2', 'QFF2', 'DD2', 'QDD2', 'FXI2', 'QFXI2', 'DXI2', 'QDXI2', 'HXI2', 'QHXI2', 'DXI3S', 'DHUMEC', 'QDHUMEC', 'GEOP', 'QGEOP', 'N', 'QN', 'NBAS', 'QNBAS', 'CL', 'QCL', 'CM', 'QCM', 'CH', 'QCH', 'N1', 'QN1', 'C1', 'QC1', 'B1', 'QB1', 'N2', 'QN2', 'C2', 'QC2', 'B2', 'QB2', 'N3', 'QN3', 'C3', 'QC3', 'B3', 'QB3', 'N4', 'QN4', 'C4', 'QC4', 'B4', 'QB4', 'W1', 'QW1', 'W2', 'QW2', 'SOL', 'QSOL', 'SOLNG', 'QSOLNG', 'TMER', 'QTMER', 'VVMER', 'QVVMER', 'ETATMER', 'QETATMER', 'DIRHOULE', 'QDIRHOULE', 'HVAGUE', 'QHVAGUE', 'PVAGUE', 'QPVAGUE', 'HNEIGEF', 'QHNEIGEF', 'TSNEIGE', 'QTSNEIGE', 'TUBENEIGE', 'QTUBENEIGE', 'HNEIGEFI3', 'QHNEIGEFI3', 'HNEIGEFI1', 'QHNEIGEFI1', 'ESNEIGE', 'QESNEIGE', 'CHARGENEIGE', 'QCHARGENEIGE', 'DIR', 'QDIR', 'DIR2', 'QDIR2', 'DIF', 'QDIF', 'DIF2', 'QDIF2', 'UV', 'QUV', 'UV2', 'QUV2', 'UV_INDICE', 'QUV_INDICE', 'INFRAR', 'QINFRAR', 'INFRAR2', 'QINFRAR2', 'TLAGON', 'QTLAGON', 'TVEGETAUX', 'QTVEGETAUX', 'ECOULEMENT', 'QECOULEMENT']\n"
     ]
    }
   ],
   "source": [
    "col_list = []\n",
    "for col in df_weather.columns:\n",
    "    num_nan = pd.isna(df_weather[col]).sum()\n",
    "    if num_nan == len(df_weather):\n",
    "        col_list.append(col)\n",
    "        print(f'Column {col} has {num_nan} missing values.')\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = df_weather.drop(columns=['FF2', 'QFF2', 'DD2', 'QDD2', 'FXI2', 'QFXI2', 'DXI2', 'QDXI2', 'HXI2', 'QHXI2', 'DXI3S',\n",
    "                                      'DHUMEC', 'QDHUMEC', 'GEOP', 'QGEOP', 'N', 'QN', 'NBAS', 'QNBAS', 'CL', 'QCL', 'CM', 'QCM',\n",
    "                                      'CH', 'QCH', 'N1', 'QN1', 'C1', 'QC1', 'B1', 'QB1', 'N2', 'QN2', 'C2', 'QC2', 'B2', 'QB2',\n",
    "                                      'N3', 'QN3', 'C3', 'QC3', 'B3', 'QB3', 'N4', 'QN4', 'C4', 'QC4', 'B4', 'QB4', 'W1', 'QW1',\n",
    "                                      'W2', 'QW2', 'SOL', 'QSOL', 'SOLNG', 'QSOLNG', 'TMER', 'QTMER', 'VVMER', 'QVVMER', 'ETATMER',\n",
    "                                      'QETATMER', 'DIRHOULE', 'QDIRHOULE', 'HVAGUE', 'QHVAGUE', 'PVAGUE', 'QPVAGUE', 'HNEIGEF', 'QHNEIGEF',\n",
    "                                      'TSNEIGE', 'QTSNEIGE', 'TUBENEIGE', 'QTUBENEIGE', 'HNEIGEFI3', 'QHNEIGEFI3', 'HNEIGEFI1', 'QHNEIGEFI1',\n",
    "                                      'ESNEIGE', 'QESNEIGE', 'CHARGENEIGE', 'QCHARGENEIGE', 'DIR', 'QDIR', 'DIR2', 'QDIR2', 'DIF', 'QDIF',\n",
    "                                      'DIF2', 'QDIF2', 'UV', 'QUV', 'UV2', 'QUV2', 'UV_INDICE', 'QUV_INDICE', 'INFRAR', 'QINFRAR', 'INFRAR2',\n",
    "                                      'QINFRAR2', 'TLAGON', 'QTLAGON', 'TVEGETAUX', 'QTVEGETAUX', 'ECOULEMENT', 'QECOULEMENT', 'NUM_POSTE',\n",
    "                                      'NOM_USUEL', 'LAT', 'LON'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[\"date\"] = pd.to_datetime(df_weather[\"AAAAMMJJHH\"], format=\"%Y%m%d%H\")\n",
    "df_weather = df_weather.drop(columns=[\"AAAAMMJJHH\", \"HXI\", \"HXY\", \"HFXI3S\", \"HTN\", \"HTX\", \"HUN\", \"HUX\"])\n",
    "\n",
    "avg = df_weather.drop(\n",
    "    columns=['RR1', 'NEIGETOT','QNEIGETOT']\n",
    "       ).groupby(\"date\").mean().reset_index()\n",
    "\n",
    "tot = df_weather.drop(\n",
    "    columns=['ALTI', 'QRR1', 'DRR1', 'QDRR1', 'FF', 'QFF', 'DD',\n",
    "       'QDD', 'FXY', 'QFXY', 'DXY', 'QDXY', 'QHXY', 'FXI', 'QFXI',\n",
    "       'DXI', 'QDXI', 'QHXI', 'FXI3S', 'QFXI3S', 'QDXI3S',\n",
    "       'QHFXI3S', 'T', 'QT', 'TD', 'QTD', 'TN', 'QTN', 'QHTN', 'TX',\n",
    "       'QTX', 'QHTX', 'DG', 'QDG', 'T10', 'QT10', 'T20', 'QT20', 'T50',\n",
    "       'QT50', 'T100', 'QT100', 'TNSOL', 'QTNSOL', 'TN50', 'QTN50',\n",
    "       'TCHAUSSEE', 'QTCHAUSSEE', 'U', 'QU', 'UN', 'QUN', 'QHUN', 'UX',\n",
    "       'QUX', 'QHUX', 'DHUMI40', 'QDHUMI40', 'DHUMI80', 'QDHUMI80',\n",
    "       'TSV', 'QTSV', 'PMER', 'QPMER', 'PSTAT', 'QPSTAT', 'PMERMIN',\n",
    "       'QPMERMIN', 'VV', 'QVV', 'DVV200', 'QDVV200', 'WW', 'QWW',\n",
    "       'GLO', 'QGLO', 'GLO2', 'QGLO2', 'INS', 'QINS', 'INS2',\n",
    "       'QINS2']\n",
    "       ).groupby(\"date\").sum().reset_index()\n",
    "\n",
    "df_weather = df_weather.drop_duplicates(subset=[\"date\"])\n",
    "\n",
    "df = df.merge(df_weather, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column DRR1 has 40991 missing values.\n",
      "Column QDRR1 has 40562 missing values.\n",
      "Column FF has 40991 missing values.\n",
      "Column QFF has 40991 missing values.\n",
      "Column DD has 40991 missing values.\n",
      "Column QDD has 40991 missing values.\n",
      "Column FXY has 40991 missing values.\n",
      "Column QFXY has 40991 missing values.\n",
      "Column DXY has 40991 missing values.\n",
      "Column QDXY has 40991 missing values.\n",
      "Column QHXY has 40991 missing values.\n",
      "Column FXI has 40991 missing values.\n",
      "Column QFXI has 40991 missing values.\n",
      "Column DXI has 40991 missing values.\n",
      "Column QDXI has 40991 missing values.\n",
      "Column QHXI has 40991 missing values.\n",
      "Column FXI3S has 40991 missing values.\n",
      "Column QFXI3S has 40991 missing values.\n",
      "Column QDXI3S has 40991 missing values.\n",
      "Column QHFXI3S has 40991 missing values.\n",
      "Column TD has 40991 missing values.\n",
      "Column QTD has 40991 missing values.\n",
      "Column T10 has 40991 missing values.\n",
      "Column QT10 has 40991 missing values.\n",
      "Column T20 has 40991 missing values.\n",
      "Column QT20 has 40991 missing values.\n",
      "Column T50 has 40991 missing values.\n",
      "Column QT50 has 40991 missing values.\n",
      "Column T100 has 40991 missing values.\n",
      "Column QT100 has 40991 missing values.\n",
      "Column TNSOL has 40991 missing values.\n",
      "Column QTNSOL has 40991 missing values.\n",
      "Column TN50 has 40991 missing values.\n",
      "Column QTN50 has 40991 missing values.\n",
      "Column TCHAUSSEE has 40991 missing values.\n",
      "Column QTCHAUSSEE has 40991 missing values.\n",
      "Column U has 40991 missing values.\n",
      "Column QU has 40991 missing values.\n",
      "Column UN has 40991 missing values.\n",
      "Column QUN has 40991 missing values.\n",
      "Column QHUN has 40991 missing values.\n",
      "Column UX has 40991 missing values.\n",
      "Column QUX has 40991 missing values.\n",
      "Column QHUX has 40991 missing values.\n",
      "Column DHUMI40 has 40991 missing values.\n",
      "Column QDHUMI40 has 40991 missing values.\n",
      "Column DHUMI80 has 40991 missing values.\n",
      "Column QDHUMI80 has 40991 missing values.\n",
      "Column TSV has 40991 missing values.\n",
      "Column QTSV has 40991 missing values.\n",
      "Column PMER has 40991 missing values.\n",
      "Column QPMER has 40991 missing values.\n",
      "Column PSTAT has 40991 missing values.\n",
      "Column QPSTAT has 40991 missing values.\n",
      "Column PMERMIN has 40991 missing values.\n",
      "Column QPMERMIN has 40991 missing values.\n",
      "Column VV has 40991 missing values.\n",
      "Column QVV has 40991 missing values.\n",
      "Column DVV200 has 40991 missing values.\n",
      "Column QDVV200 has 40991 missing values.\n",
      "Column WW has 40991 missing values.\n",
      "Column QWW has 40991 missing values.\n",
      "Column NEIGETOT has 40991 missing values.\n",
      "Column QNEIGETOT has 40991 missing values.\n",
      "Column GLO has 40991 missing values.\n",
      "Column QGLO has 40991 missing values.\n",
      "Column GLO2 has 40991 missing values.\n",
      "Column QGLO2 has 40991 missing values.\n",
      "Column INS has 40991 missing values.\n",
      "Column QINS has 40991 missing values.\n",
      "Column INS2 has 40991 missing values.\n",
      "Column QINS2 has 40991 missing values.\n",
      "['DRR1', 'QDRR1', 'FF', 'QFF', 'DD', 'QDD', 'FXY', 'QFXY', 'DXY', 'QDXY', 'QHXY', 'FXI', 'QFXI', 'DXI', 'QDXI', 'QHXI', 'FXI3S', 'QFXI3S', 'QDXI3S', 'QHFXI3S', 'TD', 'QTD', 'T10', 'QT10', 'T20', 'QT20', 'T50', 'QT50', 'T100', 'QT100', 'TNSOL', 'QTNSOL', 'TN50', 'QTN50', 'TCHAUSSEE', 'QTCHAUSSEE', 'U', 'QU', 'UN', 'QUN', 'QHUN', 'UX', 'QUX', 'QHUX', 'DHUMI40', 'QDHUMI40', 'DHUMI80', 'QDHUMI80', 'TSV', 'QTSV', 'PMER', 'QPMER', 'PSTAT', 'QPSTAT', 'PMERMIN', 'QPMERMIN', 'VV', 'QVV', 'DVV200', 'QDVV200', 'WW', 'QWW', 'NEIGETOT', 'QNEIGETOT', 'GLO', 'QGLO', 'GLO2', 'QGLO2', 'INS', 'QINS', 'INS2', 'QINS2']\n"
     ]
    }
   ],
   "source": [
    "col_list = []\n",
    "for col in df.columns:\n",
    "    num_nan = pd.isna(df[col]).sum()\n",
    "    if num_nan > 40000:\n",
    "        col_list.append(col)\n",
    "        print(f'Column {col} has {num_nan} missing values.')\n",
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['DRR1', 'FF', 'QFF', 'DD', 'QDD', 'FXY', 'QFXY', 'DXY', 'QDXY', 'QHXY', 'FXI', 'QFXI', 'DXI', 'QDXI',\n",
    "                      'QHXI', 'FXI3S', 'QFXI3S', 'QDXI3S', 'QHFXI3S', 'TD', 'QTD', 'T10', 'QT10', 'T20', 'QT20', 'T50', 'QT50',\n",
    "                      'T100', 'QT100', 'TNSOL', 'QTNSOL', 'TN50', 'QTN50', 'TCHAUSSEE', 'QTCHAUSSEE', 'U', 'QU', 'UN', 'QUN', 'QHUN',\n",
    "                      'UX', 'QUX', 'QHUX', 'DHUMI40', 'QDHUMI40', 'DHUMI80', 'QDHUMI80', 'TSV', 'QTSV', 'PMER', 'QPMER', 'PSTAT', 'QPSTAT',\n",
    "                      'PMERMIN', 'QPMERMIN', 'VV', 'QVV', 'DVV200', 'QDVV200', 'WW', 'QWW', 'NEIGETOT', 'QNEIGETOT', 'GLO', 'QGLO', 'GLO2',\n",
    "                      'QGLO2', 'INS', 'QINS', 'INS2', 'QINS2', 'QDRR1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values for weather data with their mean as there are very few\n",
    "\n",
    "is_na = df.isna().sum()\n",
    "\n",
    "for col in is_na.index:\n",
    "    if is_na[col] > 0:\n",
    "        df[col] = df[col].fillna(df[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25'], axis=1) \n",
    "y = df[['valeur_NO2', 'valeur_CO', 'valeur_O3', 'valeur_PM10', 'valeur_PM25']]  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "numerical_features = ['valeur_NO2_lag1', 'valeur_CO_lag1', 'valeur_O3_lag1',\n",
    "       'valeur_PM10_lag1', 'valeur_PM25_lag1', 'valeur_NO2_lag2',\n",
    "       'valeur_CO_lag2', 'valeur_O3_lag2', 'valeur_PM10_lag2',\n",
    "       'valeur_PM25_lag2', 'valeur_NO2_lag3', 'valeur_CO_lag3',\n",
    "       'valeur_O3_lag3', 'valeur_PM10_lag3', 'valeur_PM25_lag3']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", MinMaxScaler(), numerical_features), \n",
    "    ], remainder='passthrough').set_output(transform='pandas')\n",
    "\n",
    "data_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "data_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "data_train_prepared.to_csv(\"../processed_data/data_train_prepared.csv\", index=True)\n",
    "data_test_prepared.to_csv(\"../processed_data/data_test_prepared.csv\", index=True)\n",
    "y_train.to_csv(\"../processed_data/y_train.csv\", index=True)\n",
    "y_test.to_csv(\"../processed_data/y_test.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
